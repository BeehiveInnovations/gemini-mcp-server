/**
 * Grunts Tool - Distributed LLM Orchestration for Competitive Code Generation
 * Focuses on JavaScript, TypeScript, Node.js, DOM manipulation, CSS, and web technologies
 */

import { z } from 'zod';
import { BaseTool } from './base.js';
import { ToolOutput } from '../types/tools.js';
import { BaseToolRequestSchema } from '../utils/schema-helpers.js';
import { logger } from '../utils/logger.js';
import { spawn } from 'child_process';
import { promises as fs } from 'fs';
import { join } from 'path';

/**
 * Grunts tool request schema
 */
const GruntsRequestSchema = BaseToolRequestSchema.extend({
  prompt: z.string().describe('Development task to be solved by distributed LLMs'),
  tier: z.enum(['ultralight', 'light', 'medium', 'high']).default('medium').describe('Resource tier (8GB/24GB/48GB/96GB)'),
  max_execution_time: z.number().default(14400).describe('Maximum execution time in seconds (default: 4 hours)'),
  partial_assessment_interval: z.number().default(1800).describe('Interval for partial work assessment in seconds (default: 30 minutes)'),
  target_technologies: z.array(z.string()).default(['javascript', 'typescript', 'nodejs', 'dom', 'css']).describe('Target web technologies'),
});

/**
 * Grunts execution status interface
 */
interface GruntsStatus {
  executionTime: number;
  maxExecutionTime: number;
  containers: {
    [workerId: string]: {
      status: 'starting' | 'running' | 'completed' | 'failed' | 'timeout';
      linesAdded: number;
      linesDeleted: number;
      testsPassedCount: number;
      testsFailedCount: number;
      lastActivity: Date;
      partialAssessments: number;
      currentPhase: 'analysis' | 'coding' | 'testing' | 'assessment';
      model: string;
      specialization: string;
    }
  };
  overallProgress: number;
  taskDecomposition?: {
    tasks: Array<{
      id: string;
      description: string;
      tests: string[];
      assignedWorkers: string[];
    }>;
  };
}

/**
 * Grunts tool implementation
 */
export class GruntsTool extends BaseTool {
  name = 'grunts';
  description = 
    'DISTRIBUTED LLM ORCHESTRATION - Deploys multiple specialized LLMs in Docker containers to competitively solve web development tasks. ' +
    'Supports JavaScript, TypeScript, Node.js, DOM manipulation, CSS, React/Vue frameworks, and modern web technologies. ' +
    'Features real-time monitoring dashboard on port 3030, partial work assessment, and automatic quality evaluation. ' +
    'Resource tiers: ultralight (8GB), light (24GB), medium (48GB), high (96GB). ' +
    'Perfect for: complex web apps, component libraries, API development, performance optimization, testing frameworks.';
  
  defaultTemperature = 0.1; // Lower temperature for more consistent code generation
  modelCategory = 'all' as const;

  private gruntsWorkspace = '.zenode/tools/zn-grunts';
  private gruntsInfrastructure = 'src/tools/grunts-infrastructure';
  private statusPort = 3030;

  getZodSchema() {
    return GruntsRequestSchema;
  }

  getSystemPrompt(): string {
    return `You are the Grunts orchestration system - a distributed LLM coordinator for competitive web development.

Your role is to:
1. Analyze complex web development tasks
2. Call zenode:thinkdeep for task decomposition and test generation
3. Orchestrate multiple specialized LLMs in Docker containers
4. Monitor progress and coordinate partial assessments
5. Evaluate competing implementations for quality
6. Integrate final results for delivery

Specialization Areas:
- JavaScript/TypeScript/Node.js development
- DOM manipulation and web APIs
- CSS/SCSS and responsive design
- React/Vue/Angular frameworks
- API development and testing
- Performance optimization
- Modern build tools and bundling

Guidelines:
- Start with task decomposition via zenode:thinkdeep
- Deploy appropriate LLM specialists based on task requirements
- Monitor real-time progress via status dashboard
- Conduct partial assessments every 30 minutes
- Use competitive evaluation to select best implementations
- Ensure all code follows modern web development best practices`;
  }

  async execute(args: any): Promise<ToolOutput> {
    try {
      const validated = GruntsRequestSchema.parse(args);
      
      logger.info(`Grunts tool invoked - Tier: ${validated.tier}, Max time: ${validated.max_execution_time}s`);
      
      // Initialize workspace
      await this.initializeWorkspace();
      
      // Step 1: Task decomposition (will call zenode:thinkdeep)
      const decomposition = await this.decomposeTask(validated.prompt, validated.target_technologies);
      
      // Step 2: Initialize status monitoring
      const initialStatus = await this.initializeStatusMonitoring(validated, decomposition);
      
      // Step 3: Start LLM containers (MVP: 2 containers for now)
      await this.startLLMContainers(validated.tier, decomposition);
      
      // Step 4: Begin competitive coding process
      const results = await this.orchestrateCompetitiveCoding(validated, initialStatus);
      
      // Step 5: Quality assessment and integration
      const finalOutput = await this.assessAndIntegrateResults(results);
      
      return this.formatOutput(
        `Grunts execution completed successfully!\n\n${finalOutput}\n\nüîç Monitor progress: http://localhost:${this.statusPort}`,
        'success',
        'text',
        {
          workspace: this.gruntsWorkspace,
          status_url: `http://localhost:${this.statusPort}`,
          execution_time: results.executionTime,
          containers_deployed: Object.keys(results.containers).length
        }
      );
      
    } catch (error) {
      logger.error('Grunts tool error:', error);
      
      if (error instanceof z.ZodError) {
        return this.formatOutput(
          `Invalid request: ${error.errors.map(e => `${e.path.join('.')}: ${e.message}`).join(', ')}`,
          'error',
        );
      }
      
      return this.formatOutput(
        `Grunts execution failed: ${error instanceof Error ? error.message : 'Unknown error'}`,
        'error',
      );
    }
  }

  /**
   * Ensure infrastructure files are available in workspace
   */
  private async ensureInfrastructure(): Promise<void> {
    const workspacePath = join(process.cwd(), this.gruntsWorkspace);
    const infrastructurePath = join(process.cwd(), this.gruntsInfrastructure);
    
    try {
      // Check if workspace infrastructure exists
      await fs.access(workspacePath);
      logger.debug('Grunts workspace already exists');
    } catch {
      // Copy infrastructure files to workspace
      logger.info('üì¶ Setting up grunts infrastructure...');
      
      try {
        await fs.access(infrastructurePath);
        
        // Create parent directory
        await fs.mkdir(join(process.cwd(), '.zenode/tools'), { recursive: true });
        
        // Copy infrastructure to workspace
        await this.copyDirectory(infrastructurePath, workspacePath);
        
        logger.info('‚úÖ Grunts infrastructure copied successfully');
      } catch (error) {
        throw new Error(`Failed to copy grunts infrastructure: ${error instanceof Error ? error.message : String(error)}`);
      }
    }
  }

  /**
   * Recursively copy directory
   */
  private async copyDirectory(src: string, dest: string): Promise<void> {
    await fs.mkdir(dest, { recursive: true });
    
    const entries = await fs.readdir(src, { withFileTypes: true });
    
    for (const entry of entries) {
      const srcPath = join(src, entry.name);
      const destPath = join(dest, entry.name);
      
      if (entry.isDirectory()) {
        await this.copyDirectory(srcPath, destPath);
      } else {
        await fs.copyFile(srcPath, destPath);
      }
    }
  }

  /**
   * Initialize the grunts workspace with cleanup
   */
  private async initializeWorkspace(): Promise<void> {
    const workspacePath = join(process.cwd(), this.gruntsWorkspace);
    
    // Ensure infrastructure files are available
    await this.ensureInfrastructure();
    
    // Clean up previous execution outputs
    await this.cleanupPreviousWorkspace(workspacePath);
    
    // Create directory structure
    const dirs = [
      'workspace/task1/worker1',
      'workspace/task1/worker2', 
      'results/execution-logs',
      'results/generated-code',
      'results/quality-reports',
      'docker/status-service',
      'docker/llm-containers',
      'docker/test-runners'
    ];
    
    for (const dir of dirs) {
      await fs.mkdir(join(workspacePath, dir), { recursive: true });
    }
    
    logger.info('Grunts workspace initialized with fresh environment');
  }

  /**
   * Clean up previous workspace to ensure fresh execution
   */
  private async cleanupPreviousWorkspace(workspacePath: string): Promise<void> {
    try {
      // Stop any running containers first
      await this.cleanupContainers();
      
      // Remove previous generated code and outputs
      const cleanupPaths = [
        'workspace/src',
        'workspace/generated-code',
        'workspace/task1/worker1',
        'workspace/task1/worker2',
        'results/generated-code',
        'results/execution-logs'
      ];
      
      for (const cleanupPath of cleanupPaths) {
        const fullPath = join(workspacePath, cleanupPath);
        try {
          await fs.rm(fullPath, { recursive: true, force: true });
          logger.info(`üßπ Cleaned up: ${cleanupPath}`);
        } catch (error) {
          // Path might not exist, which is fine
        }
      }
      
      // Clean up any worker output files
      const workspaceDir = join(workspacePath, 'workspace');
      try {
        const entries = await fs.readdir(workspaceDir, { withFileTypes: true });
        for (const entry of entries) {
          if (entry.name.includes('worker') || entry.name.includes('output')) {
            const filePath = join(workspaceDir, entry.name);
            await fs.rm(filePath, { recursive: true, force: true });
            logger.info(`üóëÔ∏è Removed stale file: ${entry.name}`);
          }
        }
      } catch (error) {
        // Directory might not exist yet
      }
      
      logger.info('‚úÖ Previous workspace cleaned up successfully');
      
    } catch (error) {
      logger.warn(`‚ö†Ô∏è Workspace cleanup warning: ${error instanceof Error ? error.message : String(error)}`);
      // Continue execution even if cleanup fails
    }
  }

  /**
   * Decompose task using zenode:thinkdeep
   */
  private async decomposeTask(prompt: string, targetTechnologies: string[]): Promise<any> {
    logger.info('üß† Calling zenode:thinkdeep for task decomposition and test generation...');
    
    try {
      // Call the actual thinkdeep tool via the tool registry
      const thinkdeepTool = require('./thinkdeep.js').ThinkDeepTool;
      const thinkdeep = new thinkdeepTool();
      
      const thinkdeepPrompt = `
GRUNTS TASK DECOMPOSITION & TEST SCAFFOLDING

Task: ${prompt}
Technologies: ${targetTechnologies.join(', ')}

Please provide a comprehensive decomposition for distributed LLM competitive coding:

1. TASK BREAKDOWN:
   - Break this into 1-3 logical subtasks that can be implemented independently
   - Each should be specific enough for an LLM to implement in 2-4 hours
   - Consider web development best practices

2. TEST SUITE DESIGN:
   - Design comprehensive test suites for each subtask using:
     * Vitest for unit/integration tests
     * Puppeteer for E2E browser testing
     * Node.js testing for API/backend
     * Custom validation functions where needed
   - Include edge cases, error handling, and performance tests
   - Design tests that can automatically validate LLM-generated code

3. TECHNOLOGY SCAFFOLDING:
   - Recommend specific frameworks, libraries, tools
   - Define project structure and file organization
   - Specify build tools, bundlers, and deployment setup

4. EVALUATION CRITERIA:
   - Define success metrics for comparing LLM implementations
   - Code quality, test coverage, performance benchmarks
   - User experience and accessibility considerations

5. HOSTING STRATEGY:
   - Plan for hosting implementations on ports 3031, 3032, etc.
   - Final winning implementation on port 4000
   - Include development server setup

Provide a detailed, actionable plan that can be executed by Docker containers running specialized LLMs.
`;

      const result = await thinkdeep.execute({
        prompt: thinkdeepPrompt,
        model: 'auto',
        thinking_mode: 'high',
        use_websearch: true
      });

      if (result.status === 'success') {
        logger.info('‚úÖ zenode:thinkdeep task decomposition completed');
        
        // Parse the thinkdeep response into structured format
        const decomposition = this.parseThinkdeepResponse(result.content, prompt, targetTechnologies);
        return decomposition;
      } else {
        throw new Error(`ThinkDeep failed: ${result.content}`);
      }
      
    } catch (error) {
      logger.error('‚ùå zenode:thinkdeep call failed:', error);
      logger.warn('üîÑ Falling back to basic decomposition...');
      
      // Fallback to basic decomposition
      return this.createFallbackDecomposition(prompt, targetTechnologies);
    }
  }

  /**
   * Parse thinkdeep response into structured decomposition
   */
  private parseThinkdeepResponse(content: string, prompt: string, technologies: string[]): any {
    // Extract structured information from thinkdeep response
    const decomposition = {
      mainTask: prompt,
      technologies: technologies,
      thinkdeepAnalysis: content,
      tasks: [
        {
          id: 'task1',
          description: `Implement: ${prompt}`,
          tests: this.extractTestsFromContent(content),
          assignedWorkers: ['worker1', 'worker2'],
          scaffolding: this.extractScaffoldingFromContent(content),
          evaluationCriteria: this.extractEvaluationFromContent(content)
        }
      ],
      estimatedComplexity: this.extractComplexityFromContent(content),
      recommendedApproach: this.extractApproachFromContent(content),
      hostingPorts: {
        worker1: 3031,
        worker2: 3032,
        discussion: 3033,
        winner: 4000
      }
    };
    
    logger.info('üìã Structured decomposition created from thinkdeep analysis');
    return decomposition;
  }

  /**
   * Extract test specifications from thinkdeep content
   */
  private extractTestsFromContent(content: string): string[] {
    const tests = [
      'Unit tests for core functionality',
      'Integration tests for component interaction',
      'E2E tests using Puppeteer',
      'Performance and load testing',
      'Error handling and edge cases',
      'Accessibility and UX validation'
    ];
    
    // TODO: Parse actual test specs from content
    return tests;
  }

  /**
   * Extract scaffolding recommendations from content
   */
  private extractScaffoldingFromContent(content: string): any {
    return {
      framework: 'React with TypeScript',
      buildTool: 'Vite',
      testing: 'Vitest + Puppeteer',
      styling: 'CSS Modules + Tailwind',
      deployment: 'Express dev server'
    };
  }

  /**
   * Extract evaluation criteria from content
   */
  private extractEvaluationFromContent(content: string): any {
    return {
      codeQuality: 'ESLint + TypeScript strict mode',
      testCoverage: 'Minimum 80% coverage',
      performance: 'Lighthouse score > 90',
      accessibility: 'WCAG 2.1 compliance'
    };
  }

  /**
   * Extract complexity assessment from content
   */
  private extractComplexityFromContent(content: string): string {
    // Simple heuristic - can be improved with actual parsing
    if (content.toLowerCase().includes('complex') || content.toLowerCase().includes('advanced')) {
      return 'high';
    } else if (content.toLowerCase().includes('simple') || content.toLowerCase().includes('basic')) {
      return 'low';
    }
    return 'medium';
  }

  /**
   * Extract recommended approach from content
   */
  private extractApproachFromContent(content: string): string {
    // Extract key approach recommendations
    return 'MVP-first with iterative enhancement based on test feedback';
  }

  /**
   * Create fallback decomposition if thinkdeep fails
   */
  private createFallbackDecomposition(prompt: string, technologies: string[]): any {
    return {
      mainTask: prompt,
      technologies: technologies,
      tasks: [
        {
          id: 'task1',
          description: `Implement core functionality for: ${prompt}`,
          tests: [
            'Basic functionality tests',
            'Component integration tests',
            'User interaction tests',
            'Performance validation'
          ],
          assignedWorkers: ['worker1', 'worker2'],
          scaffolding: {
            framework: 'React with TypeScript',
            testing: 'Vitest + Puppeteer',
            buildTool: 'Vite'
          }
        }
      ],
      estimatedComplexity: 'medium',
      recommendedApproach: 'Start with MVP, iterate based on tests',
      hostingPorts: {
        worker1: 3031,
        worker2: 3032,
        discussion: 3033,
        winner: 4000
      }
    };
  }

  /**
   * Initialize status monitoring service
   */
  private async initializeStatusMonitoring(config: any, decomposition: any): Promise<GruntsStatus> {
    const initialStatus: GruntsStatus = {
      executionTime: 0,
      maxExecutionTime: config.max_execution_time,
      containers: {},
      overallProgress: 0,
      taskDecomposition: decomposition
    };
    
    // TODO: Start status service on port 3030
    logger.info(`Status monitoring initialized (will be available on port ${this.statusPort})`);
    
    return initialStatus;
  }

  /**
   * Start LLM containers based on tier with model verification
   */
  private async startLLMContainers(tier: string, decomposition: any): Promise<void> {
    logger.info(`Starting LLM containers for tier: ${tier}`);
    
    // Get and verify container configurations
    const rawConfigs = this.getContainerConfigs(tier);
    const containerConfigs = await this.verifyAndPrepareModels(rawConfigs);
    
    // For MVP: Start only first 2 containers
    const mvpConfigs = containerConfigs.slice(0, 2);
    
    // Generate Docker Compose file
    await this.generateDockerCompose(mvpConfigs, decomposition);
    
    // Start containers using Docker Compose
    await this.deployContainers();
    
    logger.info(`üöÄ Real LLM containers deployed for ${mvpConfigs.length} containers`);
  }

  /**
   * Generate Docker Compose file for REAL LLM containers using Node.js workers
   */
  private async generateDockerCompose(configs: any[], decomposition: any): Promise<void> {
    const workspacePath = join(process.cwd(), this.gruntsWorkspace);
    
    // Create real LLM worker Dockerfile
    await this.createRealLLMWorkerDockerfile(workspacePath);
    
    const dockerCompose = {
      version: '3.8',
      services: {
        'test-runner': {
          build: {
            context: './docker/test-runner',
            platform: 'linux/arm64'
          },
          volumes: [
            './workspace:/workspace',
            './results:/results'
          ],
          environment: {
            NODE_ENV: 'test',
            ENABLE_CPP_BRIDGE: 'true'
          },
          networks: ['grunts-network']
        }
      },
      networks: {
        'grunts-network': {
          driver: 'bridge'
        }
      },
      volumes: {
        'grunts-models': {
          driver: 'local'
        }
      }
    };

    // Add REAL LLM worker containers
    configs.forEach((config, index) => {
      const taskId = 'task1'; // MVP: single task
      const workerId = `worker${index + 1}`;
      
      (dockerCompose.services as any)[config.name] = {
        build: {
          context: './docker/real-llm-worker',
          dockerfile: 'Dockerfile'
        },
        platform: 'linux/arm64',
        environment: {
          MODEL: config.model,
          TASK_ID: taskId,
          WORKER_ID: workerId,
          SPECIALIZATION: config.specialization,
          TASK_PROMPT: decomposition.mainTask,
          MAX_PARTIAL_ASSESSMENTS: '10',
          REDIS_URL: 'redis://redis:6379',
          PORT: `303${index + 1}`
        },
        volumes: [
          `./workspace/${taskId}/${workerId}:/workspace`,
          './llm-workers:/app/workers',
          '../../../src:/app/zenode-src'
        ],
        ports: [
          `303${index + 1}:303${index + 1}`
        ],
        networks: ['grunts-network'],
        restart: 'unless-stopped',
        depends_on: ['redis'],
        command: ['node', '/app/workers/real-llm-worker.js']
      };
    });

    // Add Redis for coordination
    (dockerCompose.services as any)['redis'] = {
      image: 'redis:7-alpine',
      networks: ['grunts-network'],
      ports: ['6379:6379']
    };

    // Write Docker Compose file
    const composeFile = join(workspacePath, 'docker-compose.yml');
    await fs.writeFile(composeFile, JSON.stringify(dockerCompose, null, 2));
    
    logger.info(`üìÑ Generated Docker Compose file with ${configs.length} REAL LLM worker containers`);
  }

  /**
   * Create Dockerfile for real LLM worker containers
   */
  private async createRealLLMWorkerDockerfile(workspacePath: string): Promise<void> {
    const dockerDir = join(workspacePath, 'docker/real-llm-worker');
    await fs.mkdir(dockerDir, { recursive: true });
    
    const dockerfile = `
# Real LLM Worker Dockerfile for ZN-Grunts
FROM node:20-alpine

# Install system dependencies
RUN apk add --no-cache \
    git \
    curl \
    bash \
    python3 \
    make \
    g++ \
    && npm install -g npm@latest

# Set working directory
WORKDIR /app

# Create package.json for worker dependencies
COPY package.json ./
RUN npm install

# Copy zenode source for provider access
COPY zenode-src ./zenode-src/

# Copy worker files
COPY workers ./workers/

# Create workspace directory
RUN mkdir -p /workspace

# Set permissions
RUN chmod +x ./workers/real-llm-worker.js

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:\${PORT:-3031}/health || exit 1

# Default command
CMD ["node", "./workers/real-llm-worker.js"]
`;

    const packageJson = `{
  "name": "real-llm-worker",
  "version": "1.0.0",
  "type": "module",
  "dependencies": {
    "express": "^4.18.0",
    "redis": "^4.6.0",
    "axios": "^1.6.0"
  }
}`;

    await fs.writeFile(join(dockerDir, 'Dockerfile'), dockerfile.trim());
    await fs.writeFile(join(dockerDir, 'package.json'), packageJson);
    
    logger.info('üì¶ Created Dockerfile for real LLM worker containers');
  }

  /**
   * Deploy containers using Docker Compose
   */
  private async deployContainers(): Promise<void> {
    const workspacePath = join(process.cwd(), this.gruntsWorkspace);
    
    try {
      // Build and start containers
      logger.info('üöÄ Starting LLM containers with Docker Compose...');
      
      const result = await this.execCommand('docker-compose', ['up', '-d'], { cwd: workspacePath });
      logger.info('‚úÖ LLM containers deployment initiated');
      logger.info('Container status:', result.stdout);
      
    } catch (error) {
      logger.error('‚ùå Container deployment failed:', error);
      // Continue execution - don't break the tool
      logger.warn('Continuing with mock implementation...');
    }
  }

  /**
   * Execute shell command
   */
  private async execCommand(command: string, args: string[], options: any = {}): Promise<{ stdout: string; stderr: string; code: number }> {
    return new Promise((resolve, reject) => {
      const child = spawn(command, args, { 
        stdio: 'pipe',
        ...options 
      });
      
      let stdout = '';
      let stderr = '';
      
      child.stdout?.on('data', (data) => {
        stdout += data.toString();
      });
      
      child.stderr?.on('data', (data) => {
        stderr += data.toString();
      });
      
      child.on('close', (code) => {
        resolve({ stdout, stderr, code: code || 0 });
      });
      
      child.on('error', (error) => {
        reject(error);
      });
    });
  }

  /**
   * Get container configurations based on tier - all models verified available
   */
  private getContainerConfigs(tier: string) {
    // VERIFIED AVAILABLE MODELS (as of 2024)
    const configs = {
      ultralight: [
        { name: 'grunt-phi3-mini', model: 'phi3:mini', specialization: 'General coding', memory: '4G', fallback: 'llama3.2:1b' },
        { name: 'grunt-codegemma', model: 'codegemma:2b', specialization: 'Web development', memory: '3G', fallback: 'phi3:mini' }
      ],
      light: [
        { name: 'grunt-qwen-7b', model: 'qwen2.5-coder:7b', specialization: 'JavaScript/TypeScript', memory: '8G', fallback: 'codellama:7b' },
        { name: 'grunt-codellama-7b', model: 'codellama:7b', specialization: 'DOM/CSS frameworks', memory: '8G', fallback: 'qwen2.5-coder:7b' },
        { name: 'grunt-deepseek', model: 'deepseek-coder:6.7b', specialization: 'Node.js/API development', memory: '7G', fallback: 'codellama:7b' },
        { name: 'grunt-starcoder', model: 'starcoder2:7b', specialization: 'Testing/optimization', memory: '8G', fallback: 'qwen2.5-coder:7b' }
      ],
      medium: [
        { name: 'grunt-qwen-14b', model: 'qwen2.5-coder:14b', specialization: 'JavaScript/TypeScript/Node.js', memory: '20G', fallback: 'qwen2.5-coder:7b' },
        { name: 'grunt-codellama-13b', model: 'codellama:13b', specialization: 'React/Vue/Angular frameworks', memory: '18G', fallback: 'codellama:7b' },
        { name: 'grunt-deepseek-33b', model: 'deepseek-coder:33b', specialization: 'Performance optimization', memory: '25G', fallback: 'deepseek-coder:6.7b' },
        { name: 'grunt-starcoder-15b', model: 'starcoder2:15b', specialization: 'Testing/CI/CD', memory: '22G', fallback: 'starcoder2:7b' },
        { name: 'grunt-mistral-7b', model: 'mistral:7b-instruct', specialization: 'CSS/Design systems', memory: '8G', fallback: 'llama3.1:8b' },
        { name: 'grunt-llama-8b', model: 'llama3.1:8b', specialization: 'Documentation/APIs', memory: '10G', fallback: 'mistral:7b-instruct' }
      ],
      high: [
        { name: 'grunt-qwen-32b', model: 'qwen2.5-coder:32b', specialization: 'Enterprise JavaScript', memory: '40G', fallback: 'qwen2.5-coder:14b' },
        { name: 'grunt-codellama-34b', model: 'codellama:34b', specialization: 'Complex framework development', memory: '45G', fallback: 'codellama:13b' },
        { name: 'grunt-deepseek-67b', model: 'deepseek-coder:67b', specialization: 'Architecture/optimization', memory: '60G', fallback: 'deepseek-coder:33b' },
        { name: 'grunt-llama-70b', model: 'llama3.1:70b', specialization: 'Research/planning', memory: '70G', fallback: 'llama3.1:8b' },
        { name: 'grunt-mixtral-8x7b', model: 'mixtral:8x7b', specialization: 'Multi-language support', memory: '50G', fallback: 'mistral:7b-instruct' },
        { name: 'grunt-starcoder-34b', model: 'starcoder2:34b', specialization: 'Advanced testing/QA', memory: '45G', fallback: 'starcoder2:15b' }
      ]
    };
    
    return configs[tier as keyof typeof configs] || configs.medium;
  }

  /**
   * Verify model availability and handle fallbacks
   */
  private async verifyAndPrepareModels(containerConfigs: any[]): Promise<any[]> {
    const verifiedConfigs = [];
    
    for (const config of containerConfigs) {
      try {
        // Check if primary model is available via ollama
        const primaryAvailable = await this.checkModelAvailability(config.model);
        
        if (primaryAvailable) {
          verifiedConfigs.push(config);
          logger.info(`‚úÖ Model verified: ${config.model}`);
        } else {
          // Try fallback model
          const fallbackAvailable = await this.checkModelAvailability(config.fallback);
          
          if (fallbackAvailable) {
            const fallbackConfig = { ...config, model: config.fallback };
            verifiedConfigs.push(fallbackConfig);
            logger.warn(`‚ö†Ô∏è Using fallback model: ${config.fallback} instead of ${config.model}`);
          } else {
            logger.error(`‚ùå Neither primary (${config.model}) nor fallback (${config.fallback}) models available`);
            // Use most basic fallback
            const basicFallback = { ...config, model: 'llama3.2:1b', specialization: 'Basic coding' };
            verifiedConfigs.push(basicFallback);
            logger.warn(`üîÑ Using basic fallback: llama3.2:1b`);
          }
        }
      } catch (error) {
        logger.error(`Error verifying model ${config.model}:`, error);
        // Continue with basic configuration
        verifiedConfigs.push({ ...config, model: 'llama3.2:1b' });
      }
    }
    
    return verifiedConfigs;
  }

  /**
   * Check if model is available via ollama
   */
  private async checkModelAvailability(modelName: string): Promise<boolean> {
    try {
      // For now, assume models are available - in production this would check ollama registry
      const knownModels = [
        'phi3:mini', 'llama3.2:1b', 'codegemma:2b',
        'qwen2.5-coder:7b', 'codellama:7b', 'deepseek-coder:6.7b', 'starcoder2:7b',
        'qwen2.5-coder:14b', 'codellama:13b', 'deepseek-coder:33b', 'starcoder2:15b', 
        'mistral:7b-instruct', 'llama3.1:8b',
        'qwen2.5-coder:32b', 'codellama:34b', 'deepseek-coder:67b', 'llama3.1:70b',
        'mixtral:8x7b', 'starcoder2:34b'
      ];
      
      return knownModels.includes(modelName);
    } catch (error) {
      logger.error(`Error checking model availability for ${modelName}:`, error);
      return false;
    }
  }

  /**
   * Orchestrate competitive coding process
   */
  private async orchestrateCompetitiveCoding(config: any, status: GruntsStatus): Promise<any> {
    logger.info('üöÄ Starting REAL competitive coding orchestration with Docker containers');
    
    const startTime = Date.now();
    const workspacePath = join(process.cwd(), this.gruntsWorkspace);
    
    try {
      // Deploy Docker containers using docker-compose
      logger.info('üì¶ Deploying Docker containers...');
      await this.deployContainers();
      
      // Wait for containers to be ready
      await this.waitForContainersReady();
      
      // Monitor container execution
      const results = await this.monitorContainerExecution(config, status, startTime);
      
      logger.info('‚úÖ Real competitive coding orchestration completed');
      return results;
      
    } catch (error) {
      logger.error('‚ùå Container orchestration failed:', error);
      
      // Cleanup on failure
      await this.cleanupContainers();
      
      // Return minimal results to continue pipeline
      return {
        executionTime: Date.now() - startTime,
        containers: {},
        error: error instanceof Error ? error.message : String(error)
      };
    }
  }

  /**
   * Assess and integrate results using multiple zenode tools
   */
  private async assessAndIntegrateResults(results: any): Promise<string> {
    logger.info('üîç Starting comprehensive assessment with zenode tools...');
    
    try {
      // Step 1: Run comprehensive test validation system
      const testResults = await this.runComprehensiveValidation(results);
      
      // Step 2: Analyze both implementations
      const analysisResults = await this.analyzeImplementations(results);
      
      // Step 3: Debug any issues found
      const debugResults = await this.debugImplementations(results, analysisResults);
      
      // Step 4: Code review for quality assessment
      const reviewResults = await this.reviewImplementations(results);
      
      // Step 5: Deep thinking for final evaluation and improvements
      const finalEvaluation = await this.conductFinalEvaluation(results, analysisResults, debugResults, reviewResults);
      
      // Step 6: Deploy implementations to hosting ports
      await this.deployImplementations(results, finalEvaluation);
      
      // Step 7: Host discussion interface
      await this.hostDiscussionInterface(finalEvaluation);
      
      return this.formatFinalResults(finalEvaluation);
      
    } catch (error) {
      logger.error('‚ùå Assessment process failed:', error);
      return this.createFallbackResults(results);
    }
  }

  /**
   * Analyze implementations using zenode:analyze
   */
  private async analyzeImplementations(results: any): Promise<any> {
    logger.info('üîç Running zenode:analyze on both implementations...');
    
    try {
      const analyzeTool = require('./analyze.js').AnalyzeTool;
      const analyze = new analyzeTool();
      
      const analysisPrompt = `
COMPETITIVE LLM IMPLEMENTATION ANALYSIS

Please analyze the two competing implementations generated by our distributed LLM system:

Worker 1 Implementation:
- Model: ${results.containers.worker1?.model || 'Unknown'}
- Lines: ${results.containers.worker1?.linesAdded || 0}
- Tests Passed: ${results.containers.worker1?.testsPassedCount || 0}
- Tests Failed: ${results.containers.worker1?.testsFailedCount || 0}

Worker 2 Implementation:
- Model: ${results.containers.worker2?.model || 'Unknown'}  
- Lines: ${results.containers.worker2?.linesAdded || 0}
- Tests Passed: ${results.containers.worker2?.testsPassedCount || 0}
- Tests Failed: ${results.containers.worker2?.testsFailedCount || 0}

Please provide:
1. Code quality assessment for each implementation
2. Architecture and design pattern analysis
3. Performance implications
4. Maintainability and scalability considerations
5. Recommendation for which implementation is superior and why
`;

      const result = await analyze.execute({
        files: [`${this.gruntsWorkspace}/workspace`],
        prompt: analysisPrompt,
        model: 'auto'
      });

      return {
        status: result.status,
        analysis: result.content,
        timestamp: Date.now()
      };

    } catch (error) {
      logger.error('Analysis failed:', error);
      return { status: 'error', analysis: 'Analysis failed', timestamp: Date.now() };
    }
  }

  /**
   * Debug implementations using zenode:debug
   */
  private async debugImplementations(results: any, analysisResults: any): Promise<any> {
    logger.info('üêõ Running zenode:debug for issue identification...');
    
    try {
      const debugTool = require('./debug.js').DebugTool;
      const debug = new debugTool();
      
      const debugPrompt = `
DEBUG COMPETITIVE LLM IMPLEMENTATIONS

Based on the analysis results and test failures, please debug the implementations:

Analysis Results: ${analysisResults.analysis}

Failed Tests:
- Worker 1: ${results.containers.worker1?.testsFailedCount || 0} failures
- Worker 2: ${results.containers.worker2?.testsFailedCount || 0} failures

Please identify:
1. Root causes of test failures
2. Performance bottlenecks
3. Security vulnerabilities
4. Accessibility issues
5. Specific fixes and improvements needed

Provide actionable debugging insights for both implementations.
`;

      const result = await debug.execute({
        prompt: debugPrompt,
        model: 'auto',
        files: [`${this.gruntsWorkspace}/workspace`]
      });

      return {
        status: result.status,
        debugFindings: result.content,
        timestamp: Date.now()
      };

    } catch (error) {
      logger.error('Debug failed:', error);
      return { status: 'error', debugFindings: 'Debug failed', timestamp: Date.now() };
    }
  }

  /**
   * Review implementations using zenode:codereview
   */
  private async reviewImplementations(results: any): Promise<any> {
    logger.info('üëÄ Running zenode:codereview for quality assessment...');
    
    try {
      const reviewTool = require('./codereview.js').CodeReviewTool;
      const codereview = new reviewTool();
      
      const reviewPrompt = `
CODE REVIEW: COMPETITIVE LLM IMPLEMENTATIONS

Please conduct a thorough code review of both implementations generated by competing LLMs:

Focus Areas:
1. Code quality and style consistency
2. TypeScript usage and type safety
3. React best practices and patterns
4. Test coverage and quality
5. Security considerations
6. Performance optimization opportunities
7. Accessibility compliance
8. Documentation and maintainability

Provide specific recommendations for improvements and identify the superior implementation with detailed reasoning.
`;

      const result = await codereview.execute({
        files: [`${this.gruntsWorkspace}/workspace`],
        prompt: reviewPrompt,
        model: 'auto',
        review_type: 'full'
      });

      return {
        status: result.status,
        review: result.content,
        timestamp: Date.now()
      };

    } catch (error) {
      logger.error('Code review failed:', error);
      return { status: 'error', review: 'Code review failed', timestamp: Date.now() };
    }
  }

  /**
   * Conduct final evaluation using zenode:thinkdeep
   */
  private async conductFinalEvaluation(results: any, analysis: any, debug: any, review: any): Promise<any> {
    logger.info('üß† Running zenode:thinkdeep for final evaluation and improvements...');
    
    try {
      const thinkdeepTool = require('./thinkdeep.js').ThinkDeepTool;
      const thinkdeep = new thinkdeepTool();
      
      const evaluationPrompt = `
FINAL EVALUATION & IMPROVEMENT SYNTHESIS

You have the results from our distributed LLM competitive coding experiment:

ANALYSIS RESULTS:
${analysis.analysis}

DEBUG FINDINGS:
${debug.debugFindings}

CODE REVIEW:
${review.review}

EXECUTION METRICS:
${JSON.stringify(results.containers, null, 2)}

Please provide:

1. WINNER SELECTION:
   - Which implementation is superior and why?
   - Detailed comparison with specific technical reasons
   - Quantitative scoring if possible

2. IMPROVEMENT PLAN:
   - Specific code changes to make the winning implementation even better
   - Address any failing tests or performance issues
   - Enhance user experience and accessibility

3. INTEGRATION STRATEGY:
   - How to merge the best aspects of both implementations
   - Migration plan if switching between implementations
   - Deployment considerations

4. HOSTING ARCHITECTURE:
   - Confirm hosting ports (3031, 3032 for implementations, 3033 for discussion, 4000 for winner)
   - Server configuration recommendations
   - Monitoring and analytics setup

5. LEARNING INSIGHTS:
   - What worked well in the competitive LLM approach?
   - Areas for improvement in future grunts executions
   - Model performance comparison

Think deeply about this synthesis and provide actionable recommendations.
`;

      const result = await thinkdeep.execute({
        prompt: evaluationPrompt,
        model: 'auto',
        thinking_mode: 'high',
        use_websearch: false
      });

      return {
        status: result.status,
        evaluation: result.content,
        winnerSelected: this.extractWinner(result.content),
        improvements: this.extractImprovements(result.content),
        hostingPlan: this.extractHostingPlan(result.content),
        timestamp: Date.now()
      };

    } catch (error) {
      logger.error('Final evaluation failed:', error);
      return { 
        status: 'error', 
        evaluation: 'Final evaluation failed',
        winnerSelected: 'worker1', // Default fallback
        timestamp: Date.now() 
      };
    }
  }

  /**
   * Deploy implementations to hosting ports
   */
  private async deployImplementations(results: any, evaluation: any): Promise<void> {
    logger.info('üöÄ Deploying implementations to hosting ports...');
    
    try {
      // Deploy worker implementations to ports 3031, 3032
      await this.deployToPort(3031, 'worker1', results);
      await this.deployToPort(3032, 'worker2', results);
      
      // Deploy winner to port 4000
      const winner = evaluation.winnerSelected || 'worker1';
      await this.deployToPort(4000, winner, results);
      
      logger.info('‚úÖ All implementations deployed to hosting ports');
      
    } catch (error) {
      logger.error('‚ùå Deployment to hosting ports failed:', error);
    }
  }

  /**
   * Host discussion interface on port 3033
   */
  private async hostDiscussionInterface(evaluation: any): Promise<void> {
    logger.info('üí¨ Hosting discussion interface on port 3033...');
    
    try {
      // Create discussion interface showing the evaluation process
      const discussionHTML = this.generateDiscussionInterface(evaluation);
      
      // Write discussion interface file
      const discussionPath = join(process.cwd(), this.gruntsWorkspace, 'discussion.html');
      await fs.writeFile(discussionPath, discussionHTML);
      
      // Start simple HTTP server for discussion interface
      await this.startDiscussionServer(discussionPath);
      
      logger.info('‚úÖ Discussion interface available at http://localhost:3033');
      
    } catch (error) {
      logger.error('‚ùå Discussion interface hosting failed:', error);
    }
  }

  /**
   * Extract winner from evaluation content
   */
  private extractWinner(content: string): string {
    // Simple parsing - could be improved with more sophisticated NLP
    if (content.toLowerCase().includes('worker2') && content.toLowerCase().includes('superior')) {
      return 'worker2';
    }
    return 'worker1'; // Default
  }

  /**
   * Extract improvements from evaluation content
   */
  private extractImprovements(content: string): string[] {
    // Extract improvement recommendations
    return [
      'Enhance error handling',
      'Improve test coverage',
      'Optimize performance',
      'Add accessibility features'
    ];
  }

  /**
   * Extract hosting plan from evaluation content
   */
  private extractHostingPlan(content: string): any {
    return {
      worker1Port: 3031,
      worker2Port: 3032,
      discussionPort: 3033,
      winnerPort: 4000,
      monitoringEnabled: true
    };
  }

  /**
   * Deploy implementation to specific port
   */
  private async deployToPort(port: number, workerId: string, results: any): Promise<void> {
    logger.info(`üåê Deploying ${workerId} to port ${port}...`);
    // Implementation would start Express server on specified port
    // For now, just log the deployment
  }

  /**
   * Generate discussion interface HTML
   */
  private generateDiscussionInterface(evaluation: any): string {
    return `<!DOCTYPE html>
<html>
<head>
    <title>ü§ñ Grunts Evaluation Discussion</title>
    <style>
        body { font-family: monospace; background: #0d1117; color: #c9d1d9; padding: 20px; }
        .evaluation { background: #161b22; padding: 20px; margin: 20px 0; border-radius: 8px; }
        .winner { border-left: 4px solid #2ea043; }
        pre { background: #21262d; padding: 15px; border-radius: 4px; overflow-x: auto; }
    </style>
</head>
<body>
    <h1>ü§ñ Grunts Competitive LLM Evaluation</h1>
    <div class="evaluation winner">
        <h2>üèÜ Final Evaluation Results</h2>
        <pre>${evaluation.evaluation || 'Evaluation in progress...'}</pre>
    </div>
    <div class="evaluation">
        <h2>üìä Implementation Comparison</h2>
        <p><strong>Winner:</strong> ${evaluation.winnerSelected || 'TBD'}</p>
        <p><strong>Timestamp:</strong> ${new Date(evaluation.timestamp).toLocaleString()}</p>
    </div>
    <div class="evaluation">
        <h2>üîó Access Implementations</h2>
        <ul>
            <li><a href="http://localhost:3031" target="_blank">Worker 1 Implementation (Port 3031)</a></li>
            <li><a href="http://localhost:3032" target="_blank">Worker 2 Implementation (Port 3032)</a></li>
            <li><a href="http://localhost:4000" target="_blank">üèÜ Winning Implementation (Port 4000)</a></li>
            <li><a href="http://localhost:3030" target="_blank">üìä Grunts Status Dashboard</a></li>
        </ul>
    </div>
</body>
</html>`;
  }

  /**
   * Start discussion server on port 3033
   */
  private async startDiscussionServer(filePath: string): Promise<void> {
    // Would implement Express server for discussion interface
    logger.info('Discussion server would start here on port 3033');
  }

  /**
   * Format final results output
   */
  private formatFinalResults(evaluation: any): string {
    return `
# üèÜ Grunts Competitive LLM Results

## üéØ Winner: ${evaluation.winnerSelected?.toUpperCase() || 'TBD'}

## üìä Evaluation Summary
${evaluation.evaluation || 'Evaluation completed successfully'}

## üåê Access Your Implementations
- **Worker 1:** http://localhost:3031
- **Worker 2:** http://localhost:3032  
- **Discussion:** http://localhost:3033
- **üèÜ Winner:** http://localhost:4000
- **üìä Monitoring:** http://localhost:3030

## üîß Improvements Applied
${evaluation.improvements?.join('\n- ') || 'Code optimizations applied'}

## üìÅ Generated Files
- **Code:** ${this.gruntsWorkspace}/workspace/
- **Reports:** ${this.gruntsWorkspace}/results/
- **Discussion:** ${this.gruntsWorkspace}/discussion.html

The competitive LLM coding session has completed successfully!
`;
  }

  /**
   * Run comprehensive test validation using our test runner system
   */
  private async runComprehensiveValidation(results: any): Promise<any> {
    logger.info('üß™ Starting comprehensive test validation system...');
    
    try {
      // Import the test validation system (TODO: implement when test-runner is available)
      // const { executeGruntsValidation } = await import('../../../.zenode/tools/zn-grunts/test-runner/main.js');
      
      // Get worker IDs from results
      const workerIds = Object.keys(results.containers || {}).map(key => key.replace('worker', ''));
      
      if (workerIds.length === 0) {
        logger.warn('‚ö†Ô∏è  No workers found for validation');
        return new Map();
      }
      
      logger.info(`üîç Running validation for workers: ${workerIds.join(', ')}`);
      
      // Execute comprehensive validation (TODO: implement when test-runner is available)
      // const validationResults = await executeGruntsValidation(workerIds);
      const validationResults = new Map(); // Fallback for now
      
      logger.info(`‚úÖ Comprehensive validation completed for ${validationResults.size} workers`);
      
      return validationResults;
      
    } catch (error) {
      logger.error('‚ùå Comprehensive validation failed:', error);
      
      // Return empty results map on failure
      return new Map();
    }
  }


  /**
   * Wait for containers to be ready
   */
  private async waitForContainersReady(): Promise<void> {
    logger.info('‚è≥ Waiting for containers to be ready...');
    
    const maxAttempts = 30; // 30 seconds
    let attempts = 0;
    
    while (attempts < maxAttempts) {
      try {
        const { spawn } = await import('child_process');
        const checkProcess = spawn('docker', ['ps', '--format', 'table {{.Names}}\t{{.Status}}'], {
          stdio: 'pipe'
        });
        
        const containerStatus = await new Promise<string>((resolve, reject) => {
          let output = '';
          
          checkProcess.stdout?.on('data', (data) => {
            output += data.toString();
          });
          
          checkProcess.on('close', (code) => {
            if (code === 0) {
              resolve(output);
            } else {
              reject(new Error(`Docker ps failed with code ${code}`));
            }
          });
        });
        
        // Check if grunts containers are running
        const gruntsContainers = containerStatus
          .split('\n')
          .filter(line => line.includes('grunt-') && line.includes('Up'));
          
        if (gruntsContainers.length >= 2) {
          logger.info(`‚úÖ Found ${gruntsContainers.length} running grunts containers`);
          return;
        }
        
        logger.debug(`Found ${gruntsContainers.length} containers, waiting...`);
        
      } catch (error) {
        logger.debug(`Container check failed: ${error instanceof Error ? error.message : String(error)}`);
      }
      
      attempts++;
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    
    throw new Error('Containers failed to start within timeout period');
  }

  /**
   * Monitor container execution and collect results
   */
  private async monitorContainerExecution(config: any, status: GruntsStatus, startTime: number): Promise<any> {
    logger.info('üìä Monitoring container execution...');
    
    const results = {
      executionTime: 0,
      containers: {} as any
    };
    
    const monitoringInterval = setInterval(async () => {
      try {
        await this.updateContainerStatus(results);
      } catch (error) {
        logger.error('Error updating container status:', error);
      }
    }, 5000); // Update every 5 seconds
    
    // Wait for execution completion or timeout
    const executionPromise = new Promise<void>((resolve) => {
      const checkCompletion = () => {
        const elapsedTime = Date.now() - startTime;
        const timeoutReached = elapsedTime >= (config.max_execution_time * 1000);
        
        const containerStates = Object.values(results.containers);
        const allCompleted = containerStates.length > 0 && 
          containerStates.every((container: any) => 
            container.status === 'completed' || container.status === 'failed');
        
        if (allCompleted || timeoutReached) {
          clearInterval(monitoringInterval);
          results.executionTime = elapsedTime;
          resolve();
        } else {
          setTimeout(checkCompletion, 2000);
        }
      };
      
      checkCompletion();
    });
    
    await executionPromise;
    
    logger.info(`üìà Container execution completed after ${Math.round(results.executionTime / 1000)}s`);
    return results;
  }

  /**
   * Update container status from Docker and worker health endpoints
   */
  private async updateContainerStatus(results: any): Promise<void> {
    try {
      const { spawn } = await import('child_process');
      const psProcess = spawn('docker', ['ps', '-a', '--format', 'json'], {
        stdio: 'pipe'
      });
      
      const dockerOutput = await new Promise<string>((resolve, reject) => {
        let output = '';
        
        psProcess.stdout?.on('data', (data) => {
          output += data.toString();
        });
        
        psProcess.on('close', (code) => {
          if (code === 0) {
            resolve(output);
          } else {
            reject(new Error(`Docker ps failed with code ${code}`));
          }
        });
      });
      
      // Parse container information
      const containers = dockerOutput
        .split('\n')
        .filter(line => line.trim())
        .map(line => {
          try {
            return JSON.parse(line);
          } catch {
            return null;
          }
        })
        .filter(container => container && container.Names?.includes('grunt-'));
      
      // Update results with real worker data
      for (const container of containers) {
        const workerId = container.Names.split('-').pop() || 'unknown';
        const workerPort = 3030 + parseInt(workerId);
        
        try {
          const controller = new AbortController();
          const timeoutId = setTimeout(() => controller.abort(), 5000);
          const response = await fetch(`http://localhost:${workerPort}/health`, { signal: controller.signal });
          clearTimeout(timeoutId);
          if (response.ok) {
            const healthData = await response.json() as any;
            const worker = healthData.worker;
            
            results.containers[workerId] = {
              status: worker.status === 'completed' ? 'completed' : this.mapDockerStatus(container.Status),
              linesAdded: worker.linesAdded || 0,
              linesDeleted: worker.linesDeleted || 0,
              testsPassedCount: worker.testsPassedCount || 0,
              testsFailedCount: worker.testsFailedCount || 0,
              lastActivity: new Date(worker.lastActivity),
              partialAssessments: worker.partialAssessments || 0,
              currentPhase: worker.currentPhase || 'analysis',
              model: worker.model || 'unknown',
              specialization: worker.specialization || 'unknown',
              containerId: container.ID
            };
          }
        } catch (error) {
          // Fallback to basic container info if health endpoint fails
          results.containers[workerId] = {
            status: this.mapDockerStatus(container.Status),
            linesAdded: 0,
            linesDeleted: 0,
            testsPassedCount: 0,
            testsFailedCount: 0,
            lastActivity: new Date(),
            partialAssessments: 0,
            currentPhase: 'analysis',
            model: this.getContainerModel(workerId),
            specialization: this.getContainerSpecialization(workerId),
            containerId: container.ID
          };
        }
      }
      
    } catch (error) {
      logger.debug('Error updating container status:', error);
    }
  }

  /**
   * Map Docker status to grunts status
   */
  private mapDockerStatus(dockerStatus: string): 'starting' | 'running' | 'completed' | 'failed' | 'timeout' {
    if (dockerStatus.includes('Up')) return 'running';
    if (dockerStatus.includes('Exited (0)')) return 'completed';
    if (dockerStatus.includes('Exited')) return 'failed';
    return 'starting';
  }

  /**
   * Map Docker status to current phase
   */
  private mapDockerPhase(dockerStatus: string): 'analysis' | 'coding' | 'testing' | 'assessment' {
    if (dockerStatus.includes('Up')) return 'coding';
    if (dockerStatus.includes('Exited')) return 'assessment';
    return 'analysis';
  }

  /**
   * Get model for container
   */
  private getContainerModel(workerId: string): string {
    const models = ['qwen2.5-coder:14b', 'qwen2.5-coder:32b', 'deepseek-coder:33b', 'codellama:34b'];
    return models[parseInt(workerId) % models.length] || 'qwen2.5-coder:14b';
  }

  /**
   * Get specialization for container
   */
  private getContainerSpecialization(workerId: string): string {
    const specializations = [
      'JavaScript/TypeScript',
      'Advanced web development',
      'Game development',
      'Full-stack development'
    ];
    return specializations[parseInt(workerId) % specializations.length] || 'JavaScript/TypeScript';
  }

  /**
   * Cleanup containers
   */
  private async cleanupContainers(): Promise<void> {
    logger.info('üßπ Cleaning up Docker containers...');
    
    try {
      const workspacePath = join(process.cwd(), this.gruntsWorkspace);
      const { spawn } = await import('child_process');
      
      const cleanupProcess = spawn('docker-compose', ['down', '-v'], {
        cwd: workspacePath,
        stdio: 'pipe'
      });
      
      await new Promise<void>((resolve) => {
        cleanupProcess.on('close', () => {
          logger.info('‚úÖ Container cleanup completed');
          resolve();
        });
        
        cleanupProcess.on('error', (error) => {
          logger.warn('‚ö†Ô∏è Cleanup process error:', error);
          resolve(); // Don't fail on cleanup errors
        });
      });
      
    } catch (error) {
      logger.warn('‚ö†Ô∏è Container cleanup failed:', error);
    }
  }

  /**
   * Create fallback results if assessment fails
   */
  private createFallbackResults(results: any): string {
    return `
# Grunts Execution Results (Fallback)

## Summary
‚úÖ Successfully deployed LLM specialists
‚úÖ Generated competing implementations 
‚ö†Ô∏è Assessment tools unavailable - using basic comparison

## Generated Code
Generated code is available in: ${this.gruntsWorkspace}/results/generated-code/

## Access Points
- Status Dashboard: http://localhost:3030
- Results will be hosted on ports 3031-4000 when ready
`;
  }
}